---
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
editor_options: 
  markdown: 
    wrap: 72
header-includes:
  - \usepackage{geometry}
  - \geometry{top=0.75in, bottom=1in, left=1in, right=1in} # Adjust top margin
  - \usepackage{titling}
  - \pretitle{\begin{center}\LARGE\bfseries\vspace{-2cm}} # Reduce vertical space before the title
  - \posttitle{\end{center}\vspace{0.7cm}} # Adjust space after title
  - \preauthor{\begin{center}\large\bfseries\vspace{-1cm}} # Reduce space before author
  - \postauthor{\end{center}\vspace{-0.95cm}} # Reduce space after author to pull up university and email
  - \predate{\begin{center}\large\vspace{0.1cm}} # Adjust space before date to match email spacing
  - \postdate{\end{center}\vspace{0.5cm}} # Adjust space after date/email
  - \usepackage{sectsty}
  - \sectionfont{\Large}
  - \subsectionfont{\large}
  - \subsubsectionfont{\normalsize}
  - \paragraphfont{\small}
  - \subparagraphfont{\footnotesize}
---

\title{Assessing Time Series Models in Predicting Stock Market Trends}
\author{Jae Yun}
\date{University of California: Santa Barbara\\[0.05cm]jaewoong@ucsb.edu}

\maketitle

# I. Introduction

<br>

  The field of long-term stock market analysis and predictive modeling presents a compelling landscape for investigation, particularly within the technology sector where entities such as Amazon, Microsoft, Nvidia, and Google have established dominance. These corporations not only boast substantial market capitalization but have also exhibited pronounced growth over the last decade, outpacing other industries and securing their position at the forefront of the economic arena. However, creating a predictive model that accurately captures the stock market's future prices is not a simple endeavor. It requires not only a thorough understanding of the dynamics of the market, but also a high dependency on market sentiment, financial domain knowledge, external influences, and various uncertainties that drive the market.

  This study selects these four technology giants as the focal point, driven by a desire to critically evaluate the performance of some of the largest companies within a homogeneous sector. By comparing stocks that are within the same industry, the analysis aims to discern whether model effectiveness varies significantly with the variance in individual stock returns, regardless of non-dynamic factors. This approach facilitates an in-depth understanding of how well the model captures sector-specific dynamics and whether its predictive accuracy is consistent across different yet closely related stocks. Essentially, the goal is to determine the model's robustness in forecasting within a singular sector, providing insights into its utility and reliability in predicting stock trends in the technology industry.

  The objective of this analysis is to apply time series forecasting models to the stock prices of Amazon, Microsoft, Nvidia, and Google, spanning a decade of 552 weekly opening price observations from January 1st, 2010, to January 1st, 2020. The focus is to evaluate the efficacy of these models, utilizing only the market trends and seasonal patterns, and intentionally excluding external, macroeconomic events such as the COVID-19 pandemic, Ukrainian-Russian War, The Great Recession, and other major economic crises which could skew the data and compromise the integrity of the technical-only focused forecast.

  The rationale for this approach stems from a desire to ascertain the intrinsic predictability of stock prices based on historical data alone, thus evaluating the robustness of time series models as a tool for financial forecasting. By delineating the influence of major unpredictable occurrences, the study aims to distill the efficacy of time series analysis in capturing the underlying patterns and tendencies inherent to the stock prices of these leading technology firms.

  To this end, study will transform our data to a stationary process in order to determine the parameters to employ a time series forecasting model, called the SARIMA (Seasonal ARIMA) model, to project future stock prices of Amazon, Microsoft, Nvidia, and Google. The effectiveness of these models will be gauged through the data from 2010 to 2018 to forecast stock prices for the year 2019. The predictive accuracy will be evaluated based on how closely the forecasted values align with the actual stock prices observed in 2019, thus providing a measure of the models' performance using AIC (Akaike Information Criterion) and visual representations of confidence intervals to assess predictive accuracy.

  This analysis seeks to not only test the validity of time series forecasting in a stock market context but also to explore the potential of these models as a significant component of a strategic investment framework. The study intends to evaluate how well time series models can perform in a generally volatile stock market setting.

<br>

# II. Key Terms

<br>

### Augmented Dickey-Fuller Test (ADF Test)

The ADF Test is a common statistical test to determine if a time series is stationary. This is an important assumption, as SARIMA models must assume that a time series is stationary in order to accurately estimate the model parameters and provide reliable forecasts. A time series is considered stationary if its statistical properties, such as mean, variance, and autocorrelation, are constant over time. The ADF test is given by the expression[^1]

$$\Delta y_t = \alpha + \beta t + \gamma y_{t-1} + \delta_1 \Delta y_{t-1} + \cdots + \delta_{p-1} \Delta y_{t-p+1} + \varepsilon_t$$

where $\Delta y_t$ is the first difference of the time series at time
$t$, $\alpha$ is a constant term, $\beta$ is the coefficient of a linear
time trend, and $\gamma$ is the coefficient of the lagged value
$y_{t-1}$. The terms $\delta_i$ are the coefficients of the lagged first
differences, up to a maximum lag of $p-1$. The error term
$\varepsilon_t$ is assumed to be white noise.

<br>

### Kwiatkowski-Phillips-Schmidt-Shin (KPSS) Test

Much like the Augmented Dickey-Fuller (ADF) Test, the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test is a statistical method used to analyze the stationarity of a time series. While the ADF test focuses on identifying a unit root indicating non-stationarity, the KPSS test takes the opposite approach by testing the null hypothesis that the time series is stationary around a deterministic trend. It's an essential tool for validating the stationarity of a series before fitting models like SARIMA, which require a stationary input.

The KPSS test checks for stationarity by assessing the presence of a stochastic trend, and is formulated as follows[^2]:

$$
\text{KPSS} = \frac{S_{T}^2}{T \hat{\sigma}^2}
$$

where:
- $S_{T}^2$ is the sum of squared deviations of the cumulative sum of residuals (or deviations from the trend),
- $T$ is the total number of observations,
- $\hat{\sigma}^2$ is an estimate of the variance of the residuals.

In contrast to the ADF test, which subtracts lagged values of the time series to find evidence of non-stationarity, the KPSS test sums the squared deviations to test the strength of the trend component. A small p-value in the ADF test suggests non-stationarity, whereas in the KPSS test, a small p-value indicates stationarity. In practice, both tests are used in conjunction to provide a more robust assessment of the time series stationarity.

<br>

### Box-Cox Transformation

The Box-Cox transformation is a statistical technique employed to normalize the distribution of a time series and stabilize variance, particularly when the data exhibits non-constant variance (heteroscedasticity). It can also help in making the residuals of a model more homoscedastic and normally distributed, which is an important step in meeting the stationarity requirements necessary for accurate time series forecasting. The transformation is particularly beneficial when the assumption of equal variance is violated. The Box-Cox transformation is defined as follows:

For a time series $y_t$ and a transformation parameter $\lambda$, the Box-Cox transformed series $y_t'$ is computed using:

$$
y_t' = 
\begin{cases} 
\frac{y_t^\lambda - 1}{\lambda} & \text{if } \lambda \neq 0, \\
\log(y_t) & \text{if } \lambda = 0. 
\end{cases}
$$

Here:
- $y_t$ is the observed value at time $t$.
- $\lambda$ is the Box-Cox transformation parameter, which is determined empirically to best stabilize the variance of the time series.
- $y_t'$ represents the transformed value at time $t$.

The parameter $\lambda$ is typically estimated to maximize the likelihood function, and it can vary across a wide range. When $\lambda = 1$, the transformation is equivalent to no transformation (the identity function). When $\lambda = 0$, the Box-Cox transformation corresponds to taking the natural logarithm of $y_t$. The goal is to find the value of $\lambda$ that results in a time series with constant variance and approximately normal residuals.

Once transformed, the series should be assessed for improved homoscedasticity and normality. Subsequent time series analyses, including ARIMA modeling, are conducted on the transformed data. Predictions or forecasts obtained from the model should be back-transformed to the original scale, which can be done using the inverse Box-Cox transformation.


<br>

### Kwiatkowski-Phillips-Schmidt-Shin (KPSS) Test

The Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test,  is a statistical procedure used to assess the stationarity of a time series. Unlike tests that check for the presence of a unit root, the KPSS test examines the null hypothesis that a time series is stationary around a deterministic trend (level stationarity) or around a stochastic trend (trend stationarity). This test is particularly useful when a time series exhibits fluctuations around a drifting mean or an evolving trend. The KPSS test is defined as:

For a time series $y_t$, the KPSS statistic is computed as:

$$
\text{KPSS} = \frac{\sum_{t=1}^{T} \hat{S}_t^2}{T^2 \hat{\sigma}^2}
$$

where:
- $T$ is the number of observations in the time series.
- $\hat{S}_t$ is the partial sum of the residuals (or deviations from the mean for level stationarity, or from the trend for trend stationarity), up to time $t$.
- $\hat{\sigma}^2$ is an estimate of the long-run variance of the time series.

The null hypothesis ($H_0$) of the KPSS test is that the time series is stationary. A small p-value (typically less than 0.05) indicates that the null hypothesis can be rejected, suggesting the presence of a unit root, or in other words, that the time series is non-stationary. A large p-value suggests that the time series is stationary.


<br>

### Time Series Decomposition

Decomposition is a method used in time series analysis to break down a time series (Y(t)) into several components, typically including trend, seasonal, and random or residual components. The random/residual component may result in a stationary time series. This breakdown helps in understanding, analyzing, and forecasting the data more effectively. The formula is as follows[^1] 

\[Y(t) = T(t) + S(t) + R(t)\]

- \(Y(t)\) is the observed value of the time series at time \(t\), given by \(T(t) + S(t) + R(t)\).

- \(T(t)\) is the trend component of the time series at time \(t\), represented as \(T_t = f(t)\).

- \(S(t)\) is the seasonal component of the time series at time \(t\), calculated as \(S_t = \sum_{i=1}^n s_i\).

- \(R(t)\) is the residual or random component of the time series at time \(t\), defined as \(R_t = Y_t - T_t - S_t\).

<br>

### Autocorrelation function (ACF)

The Autocorrelation Function (ACF) is a fundamental tool in time series analysis that measures the linear relationship between an observation in a time series and the observations at previous times (lags). By looking at the ACF plot, you can determine how many past values (lags) in the time series significantly influence the current value. This is crucial for identifying the 'Moving Average' (MA) component in ARIMA models.

The ACF is given by the expression[^1] 

$$\text{ACF}(k) = \frac{\sum_{t=k+1}^n (Y_t - \bar{Y})(Y_{t-k} - \bar{Y})}{\sum_{t=1}^n (Y_t - \bar{Y})^2}$$

where $k$ is the lag, $n$ is the number of observations in the time
series, $Y_t$ is the observed value at time $t$, and $\bar{Y}$ is the
sample mean of the time series.

<br>

### Partial Autocorrelation Function (PACF)

The Partial Autocorrelation Function (PACF) offers a measure of the correlation between observations in a time series separated by various time lags, while controlling for the correlations at shorter lags. PACF is key in identifying the 'Autoregressive' (AR) component in ARIMA models. It indicates the extent of the direct relationship between an observation and its lag.

The PACF is expressed as[^1] 

$$\rho_k = \frac{\text{cov}(Y_t, Y_{t-k})}{\text{var}(Y_t)} - \sum_{j=1}^{k-1} \phi_j \rho_{k-j}$$

where $\rho_k$ is the partial autocorrelation at lag $k$,
$\text{cov}(Y_t, Y_{t-k})$ is the covariance between $Y_t$ and
$Y_{t-k}$, $\text{var}(Y_t)$ is the variance of $Y_t$, $\phi_j$ is the
estimated coefficient of the autoregressive (AR) model at lag $j$, and
$k$ is the maximum lag for which the PACF is computed.

<br>

### SARIMA Model

The SARIMA (Seasonal Autoregressive Integrated Moving Average) model is an advanced form of the ARIMA model. It's specifically designed to better model and forecast time series data that show seasonality – regular fluctuations or patterns that repeat over time. Seasonality can be observed in many real-world data such as monthly sales, daily temperature readings, weekly website traffic, or in our case – the stock market.

The SARIMA Model is formulated as follows[^1] 

$$(1 - \phi_1 B - \cdots - \phi_p B^p)(1 - \Phi_1 B^s - \cdots - \Phi_P B^{sP})^d (y_t - \mu)$$
$$= (1 + \theta_1 B + \cdots + \theta_q B^q)(1 + \Theta_1 B^s + \cdots + \Theta_Q B^{sQ})\varepsilon_t$$
where $y_t$ is the observed value of the time series at time $t$, $\mu$
is the mean of the time series, $\varepsilon_t$ is the error term at
time $t$, and $B$ is the backshift operator. The parameters $p$, $d$,
and $q$ correspond to the order of the autoregressive (AR), integrated
(I), and moving average (MA) components, respectively. The parameters
$P$, $D$, and $Q$ correspond to the order of the seasonal AR, seasonal
I, and seasonal MA components, respectively. The parameter $s$
represents the length of the seasonal cycle.

The $\phi$ and $\theta$ parameters are the coefficients of the AR and MA
terms, respectively, while the $\Phi$ and $\Theta$ parameters are the
coefficients of the seasonal AR and seasonal MA terms

[^1]: Shumway, R. H., & Stoffer, D. S. (2017). *Time Series Analysis and Its Applications: With R Examples*. Springer Texts in Statistics, 4th ed.

<br>

### Ljung-Box Test

The Ljung-Box Test is widely used type of statistical test primarily used to determine the absence of autocorrelation at multiple lag lengths in residuals from a time series analysis. The essence of the test is to verify the randomness of a time series, which is a crucial assumption in time series modeling, and assessing the SARIMA model's adequacy.

The Ljung-Box Test is expressed as[^1]

$$Q = n(n+2) \sum_{k=1}^h \frac{\hat{\rho}_k^2}{n-k}$$


where $Q$ is the test statistic, $n$ is the sample size, $h$ is the
number of lags being tested, and $\hat{\rho}_k$ is the sample
autocorrelation at lag $k$. The null hypothesis of the test is that the
autocorrelations up to lag $h$ are equal to zero, indicating that the
time series is a white noise process. The alternative hypothesis states that
the autocorrelations are not equal to zero, indicating the presence of
serial correlation in the data. The test statistic $Q$ follows a
chi-squared distribution with $h-p$ degrees of freedom, where $p$ is the
number of parameters estimated in the time series model.

<br>

### Box-Pierce Test

The Box-Pierce Test is another well-known statistical test used to check the independence of residuals in a time series model. It is similar to the Ljung-Box Test but predates it and is based on a simpler formulation. The primary purpose of this test is to detect any autocorrelation in the residuals of a fitted time series model, thereby assessing the SARIMA model's adequacy.

The Box-Pierce is formulated as[^1] 

$$Q^* = n \sum_{k=1}^h \hat{\rho}_k^2$$

where $Q^*$ is the test statistic, $n$ is the sample size, $h$ is the
number of lags being tested, and $\hat{\rho}_k$ is the sample
autocorrelation at lag $k$. The null hypothesis of the test is that the
autocorrelations up to lag $h$ are equal to zero, indicating that the
time series is a white noise process. The alternative hypothesis states that
the autocorrelations are not equal to zero, indicating the presence of
serial correlation in the data.

The test statistic $Q^*$ follows an approximately chi-squared
distribution with $h-p$ degrees of freedom, where $p$ is the number of
parameters estimated in the time series model.

<br>

# III. Data

<br>

In our time series dataset, we observe five key dimensions, commonly referred to as the 5V's, that significantly enhance the depth and utility of our analysis:

-   **Volume**: Our dataset encompasses a substantial volume of data, spanning ten years of weekly stock market information. This extensive collection of data lays a solid foundation for making precise and effective predictions.

-   **Velocity**: The dataset's velocity is characterized by its weekly frequency, providing a steady and consistent flow of information for timely analysis.

-   **Variety**: Despite focusing on just four major stocks within the technology sector, our dataset is rich in variety. Each stock represents distinct market behaviors and price patterns, unique to each company, offering a diverse perspective.

-   **Veracity**: The data, sourced from Yahoo Finance, is marked by its high level of veracity. Reflecting actual stock market prices, this dataset offers exceptional accuracy, crucial for reliable analysis.

-   **Value**: Analyzing this data allows us to identify market trends and perform in-depth analysis. Such insights are invaluable not only for understanding past market behaviors but also for forecasting future trends. This is particularly significant given the influential nature of these technology stocks in the broader economic context.

Now, we can begin using R to import our data from yahoo, read csv, and plot the
time series data:

```{r, echo = F, warning = F, include = F}
library(dplyr)
library(forecast)
library(astsa)
library(ROCR)
library(tseries)
library(ggplot2)
knitr::opts_chunk$set(fig.width=12, fig.height=6) 
```

```{r, echo = F}
NVDA_Weekly <- readr::read_csv("~/Downloads/NVDA.csv",show_col_types = FALSE)
GOOGL_Weekly <- readr::read_csv("~/Downloads/GOOGL.csv",show_col_types = FALSE)
MSFT_Weekly <- readr::read_csv("~/Downloads/MSFT.csv",show_col_types = FALSE)
AMZN_Weekly <- readr::read_csv("~/Downloads/AMZN.csv",show_col_types = FALSE)


df_nvda <- data.frame(Date = NVDA_Weekly$Date, Open = NVDA_Weekly$Open)
df_googl <- data.frame(Date = GOOGL_Weekly$Date, Open = GOOGL_Weekly$Open)
df_msft <- data.frame(Date = MSFT_Weekly$Date, Open = MSFT_Weekly$Open)
df_amzn <- data.frame(Date = AMZN_Weekly$Date, Open = AMZN_Weekly$Open)


# Split Data into Training and Test Sets
googl_train <- df_googl[c(1:470),]
googl_test <- df_googl[c(471:522),]

nvda_train <- df_nvda[c(1:470),]
nvda_test <- df_nvda[c(471:522),]

msft_train <- df_msft[c(1:470),]
msft_test <- df_msft[c(471:522),]

amzn_train <- df_amzn[c(1:470),]
amzn_test <- df_amzn[c(471:522),]


# Turn into time series
gtr <- ts(googl_train$Open, frequency = 52, start = c(2010, 1))
gte <- ts(googl_test$Open, frequency = 52, start = c(2019, 1))

ntr <- ts(nvda_train$Open, frequency = 52, start = c(2010, 1))
nte <- ts(nvda_test$Open, frequency = 52, start = c(2019, 1))

mtr <- ts(msft_train$Open, frequency = 52, start = c(2010, 1))
mte <- ts(msft_test$Open, frequency = 52, start = c(2019, 1))

atr <- ts(amzn_train$Open, frequency = 52, start = c(2010, 1))
ate <- ts(amzn_test$Open, frequency = 52, start = c(2019, 1))
```

```{r, include = F}
GOOGL_Weekly_Open_ts <- ts(GOOGL_Weekly$Open, start = c(2010, 1), frequency = 52)
NVDA_Weekly_Open_ts <- ts(NVDA_Weekly$Open, start = c(2010, 1), frequency = 52)
MSFT_Weekly_Open_ts <- ts(MSFT_Weekly$Open, start = c(2010, 1), frequency = 52)
AMZN_Weekly_Open_ts <- ts(AMZN_Weekly$Open, start = c(2010, 1), frequency = 52)
```

```{r, echo = F}
par(mfrow = c(2, 2))

# Define a professional color
professional_color = "lightgray"

# Histogram for Nvidia
hist(ntr, main = "Nvidia Weekly Open Share Price", 
     xlab = "Price", ylab = "Frequency", 
     col = professional_color, border = "black")

# Histogram for Google
hist(gtr, main = "Google Weekly Open Share Price", 
     xlab = "Price", ylab = "Frequency", 
     col = professional_color, border = "black")

# Histogram for Microsoft
hist(mtr, main = "Microsoft Weekly Open Share Price", 
     xlab = "Price", ylab = "Frequency", 
     col = professional_color, border = "black")

# Histogram for Amazon
hist(atr, main = "Amazon Weekly Open Share Price", 
     xlab = "Price", ylab = "Frequency", 
     col = professional_color, border = "black")


```

<br> 

Looking at the trends of each weekly opening price for our 4 technology stocks, we see a steady increase from 2010 to 2016, and a sharp, yet fluctuating increase from 2016 to 2020. 

```{r, echo = F, include = F}
# Checking for NA values in our dataset:
data.frame(googl_isna = sum(is.na(GOOGL_Weekly$Open)),
           nvda_isna = sum(is.na(NVDA_Weekly$Open)),
           msft_isna = sum(is.na(MSFT_Weekly$Open)),
           amzn_isna = sum(is.na(AMZN_Weekly$Open)))
```

<br>

These plots show the presence of non-stationarity and non-randomness within our datasets and thus, we must perform differencing or decomposition to accurately utilize our forecasting model. We can also construct histograms to better visualize the distribution of our current non-stationary dataset:

```{r, echo = F}
adf_results <- data.frame(
  Company = c("Google", "Nvidia", "Microsoft", "Amazon"),
  ADF_P_Value = c(
    adf.test(gtr)$p.value,
    adf.test(ntr)$p.value,
    adf.test(mtr)$p.value,
    adf.test(atr)$p.value
  )
)

# Adjusting plot dimensions
small_plot_width <- 6 # in inches
small_plot_height <- 4 # in inches

# Creating a smaller bar plot
ggplot(adf_results, aes(x = Company, y = ADF_P_Value, fill = Company)) +
  geom_bar(stat = "identity", color = "black", fill = "lightgray") +
  theme_minimal() +
  labs(title = "ADF Test P-Values for Different Companies",
       x = "Company",
       y = "ADF Test P-Value") +
  geom_hline(yintercept = 0.05, linetype = "dashed", color = "red") +
  theme(legend.position = "none") +
  theme(plot.margin = unit(c(1, 1, 1, 1), "cm")) # Adjust margins around plot
```

Using the ADF test, we extract the p-value for each dataset. Each p-value exceeds the significance value represented as the red dotted line where p = 0.05. Consequently, we fail to reject the null hypothesis and conclude that we do not have sufficient evidence to determine that each data is non-stationary.

<br>

# IV. Data Augmentation

Through iterative examination during our study, it became evident that decomposition alone was insufficient for rendering the random component of dataset, and thus we first transformed the dataset using the Box-Cox transformation. This transformation helps in mitigating the effects of non-stationarity related to non-constant variance, which is a common characteristic of financial time series data. By doing so, we aim to transform the series into one that has a stable variance and a more symmetrical distribution, thereby creating a better foundation for the subsequent decomposition to isolate and remove the trend and seasonal components effectively.

We can then plot a decomposition of the Box-Cox, to see the Box-Cox observed values, and see if there are any trends or seasonality patterns.

```{r, echo = F}
# Applying Box-Cox transformation to the Google time series
gtr_bc_lambda = BoxCox.lambda(gtr)
gtr_boxcoxed = BoxCox(gtr, lambda = gtr_bc_lambda)
plot(decompose(gtr_boxcoxed))

# Applying Box-Cox transformation to the Nvidia time series
ntr_bc_lambda = BoxCox.lambda(ntr)
ntr_boxcoxed = BoxCox(ntr, lambda = ntr_bc_lambda)
plot(decompose(ntr_boxcoxed))

# Applying Box-Cox transformation to the Microsoft time series
mtr_bc_lambda = BoxCox.lambda(mtr)
mtr_boxcoxed = BoxCox(mtr, lambda = mtr_bc_lambda)
plot(decompose(mtr_boxcoxed))

# Applying Box-Cox transformation to the Amazon time series
atr_bc_lambda = BoxCox.lambda(atr)
atr_boxcoxed = BoxCox(atr, lambda = atr_bc_lambda)
plot(decompose(atr_boxcoxed))

```

<br>

The examination of the random components across all four Box-Cox transformed time series suggests that they closely approximate a white noise distribution. This resemblance indicates that the random components are stationary, providing a solid foundation for the identification of parameters for our SARIMA models with greater confidence. To confirm further, we can perform the ADF test on each decomposed random component of the box-cox transformed data.


```{r, echo = F}
googl_residuals <- na.omit(decompose(gtr_boxcoxed)$random)
nvda_residuals <- na.omit(decompose(ntr_boxcoxed)$random)
msft_residuals <- na.omit(decompose(mtr_boxcoxed)$random)
amzn_residuals <- na.omit(decompose(atr_boxcoxed)$random)
```

Here are our plots for our Box-Cox transformed residual-extracted components to check for stationarity.

```{r, echo = F}
par(mfrow = c(2, 2))

# Plot for Google Residuals
plot(googl_residuals, main = "Google Box-Cox Transformed Residuals", ylab = "Residuals")
abline(h = 0, col = "red")

# Plot for Nvidia Residuals
plot(nvda_residuals, main = "Nvidia Box-Cox Transformed Residuals", ylab = "Residuals")
abline(h = 0, col = "red")

# Plot for Microsoft Residuals
plot(msft_residuals, main = "Microsoft Box-Cox Transformed Residuals", ylab = "Residuals")
abline(h = 0, col = "red")

# Plot for Amazon Residuals
plot(amzn_residuals, main = "Amazon Box-Cox Transformed Residuals", ylab = "Residuals")
abline(h = 0, col = "red")

```

It seems we have achieved stationarity within our twice-augmented dataset. We can also test the statistical hypothesis for stationarity using the ADF test.

```{r, echo = F, warning = F}
adf_results <- data.frame(
Company = c(
  "Google",
  "Nvidia",
  "Microsoft",
  "Amazon"),

ADF_P_Value = c(
    adf.test(googl_residuals)$p.value,
    adf.test(nvda_residuals)$p.value,
    adf.test(msft_residuals)$p.value,
    adf.test(amzn_residuals)$p.value
  )
)

# Adjusting plot dimensions
small_plot_width <- 6 # in inches
small_plot_height <- 4 # in inches

# Creating a smaller bar plot
ggplot(adf_results, aes(x = Company, y = ADF_P_Value, fill = Company)) +
  geom_bar(stat = "identity", color = "black", fill = "lightgray") +
  theme_minimal() +
  labs(title = "ADF Test P-Values for Different Companies",
       x = "Company",
       y = "ADF Test P-Value") +
  geom_hline(yintercept = 0.05, linetype = "dashed", color = "red") +
  theme(legend.position = "none") +
  theme(plot.margin = unit(c(1, 1, 1, 1), "cm")) # Adjust margins around plot
```

Our ADF Test P-values for each twice-augmented datasets are now below the significance level of 0.05, and thus we reject the null hypothesis for non-stationarity. Therefore, we conclude with significant evidence that each of our time series data are stationary. Consequently, we can move on with the next steps in selecting parameters in our SARIMA model using PACF and ACF plots.

# V Models and Parameter Selection

<br>

```{r, cache = T, echo = F}
library(forecast)
```

We will employ the SARIMA model to evaluate performance, selecting the optimal model based on the lowest Akaike Information Criterion (AIC) values. Model parameters will be determined through analysis of the ACF and PACF plots.

Considering SARIMA model follows parameters Sarima(p, d, q, P, D, Q), we will use this in our evaluation.

```{r, cache = T, echo = F}
# Set up the plotting area to have 2 rows and 4 columns
par(mfrow=c(2,2))

# ACF and PACF for Google residuals
acf(googl_residuals, lag.max = 104, main="ACF for Google")
pacf(googl_residuals, lag.max = 104, main="PACF for Google")

# ACF and PACF for Nvidia residuals
acf(nvda_residuals, lag.max = 104, main="ACF for Nvidia")
pacf(nvda_residuals, lag.max = 104, main="PACF for Nvidia")

# ACF and PACF for Microsoft residuals
acf(msft_residuals, lag.max = 104, main="ACF for Microsoft")
pacf(msft_residuals, lag.max = 104, main="PACF for Microsoft")

# ACF and PACF for Amazon residuals
acf(amzn_residuals, lag.max = 104, main="ACF for Amazon")
pacf(amzn_residuals, lag.max = 104, main="PACF for Amazon")

```

- A significant spike at lag 1 (outside the confidence interval) may suggest an AR(1) component (p = 1) for the non-seasonal part of the model.

- The need for differencing (d) may not be necessary since the tests indicate the series is already stationary (d=0).

- The exact number (q) can be challenging to determine solely from the ACF plot when there are many significant lags.

- The sine-wave like features represent our data still contains a seasonal pattern. Thus, we need to difference it further using seasonal differencing. Thus we will set D = 1, and set P = 0 or 1, and Q = 0 or 1.

Thus, we choose a SARIMA model with parameters SARIMA($p = 1$, $d = 0$, $q = 1$ or $2$, $P = 0$ or $1$, $D = 1$, $Q = 0$ or $1$).

- We also see very similar patterns for the other ACF and PACF plots  

The chosen SARIMA model parameters for the NVIDIA, Microsoft, and Amzon residuals could be SARIMA($p = 1$, $d = 0$, $q = 1$ or $2$, $P = 0$ or $1$, $D = 1$, $Q = 0$ or $1$).




```{r, cache = T, include = F, echo = F}
knitr::opts_chunk$set(cache = TRUE)


# Google models
googl_model1 <- arima(googl_residuals, order=c(1,0,2), seasonal=list(order=c(1,1,0), period=52), include.mean=FALSE)
googl_model2 <- arima(googl_residuals, order=c(1,0,2), seasonal=list(order=c(0,1,1), period=52), include.mean=FALSE)
googl_model3 <- arima(googl_residuals, order=c(1,0,2), seasonal=list(order=c(1,1,1), period=52), include.mean=FALSE)

# Nvidia models
nvda_model1 <- arima(nvda_residuals, order=c(1,0,2), seasonal=list(order=c(1,1,0), period=52), include.mean=FALSE)
nvda_model2 <- arima(nvda_residuals, order=c(1,0,2), seasonal=list(order=c(0,1,1), period=52), include.mean=FALSE)
nvda_model3 <- arima(nvda_residuals, order=c(1,0,2), seasonal=list(order=c(1,1,1), period=52), include.mean=FALSE)

# Microsoft models
msft_model1 <- arima(msft_residuals, order=c(1,0,2), seasonal=list(order=c(1,1,0), period=52), include.mean=FALSE)
msft_model2 <- arima(msft_residuals, order=c(1,0,2), seasonal=list(order=c(0,1,1), period=52), include.mean=FALSE)
msft_model3 <- arima(msft_residuals, order=c(1,0,2), seasonal=list(order=c(1,1,1), period=52), include.mean=FALSE)

# Amazon models
amzn_model1 <- arima(amzn_residuals, order=c(1,0,2), seasonal=list(order=c(1,1,0), period=52), include.mean=FALSE)
amzn_model2 <- arima(amzn_residuals, order=c(1,0,2), seasonal=list(order=c(0,1,1), period=52), include.mean=FALSE)
amzn_model3 <- arima(amzn_residuals, order=c(1,0,2), seasonal=list(order=c(1,1,1), period=52), include.mean=FALSE)
```

```{r, cache = T, echo = F}
google_nvda_aic <- data.frame(
  Google_Model_1 = googl_model1$aic,
  Google_Model_2 = googl_model2$aic,
  Google_Model_3 = googl_model3$aic,
  Nvidia_Model_1 = nvda_model1$aic,
  Nvidia_Model_2 = nvda_model2$aic,
  Nvidia_Model_3 = nvda_model3$aic)

msft_amzn_aic<-data.frame(
  Microsoft_Model_1 = msft_model1$aic,
  Microsoft_Model_2 = msft_model2$aic,
  Microsoft_Model_3 = msft_model3$aic,
  Amazon_Model_1 = amzn_model1$aic,
  Amazon_Model_2 = amzn_model2$aic,
  Amazon_Model_3 = amzn_model3$aic
)

# Add titles to the headers
names(google_nvda_aic) <- c("Google Model 1 AIC", "Google Model 2 AIC", "Google Model 3 AIC", 
                            "Nvidia Model 1 AIC", "Nvidia Model 2 AIC", "Nvidia Model 3 AIC")

names(msft_amzn_aic) <- c("Microsoft Model 1 AIC", "Microsoft Model 2 AIC", "Microsoft Model 3 AIC", 
                          "Amazon Model 1 AIC", "Amazon Model 2 AIC", "Amazon Model 3 AIC")

google_nvda_aic

msft_amzn_aic
```


<br>

# VI. Model Evaluation on Train Data

<br>

Our residuals should look like a normal white noise to represent an adequate model fit. We can test this using histograms.

```{r, echo = F}
# Test for googl:
par(mfrow=c(3,2))
hist(googl_model1$residuals, breaks = 30)
hist(googl_model2$residuals, breaks = 30)
hist(googl_model3$residuals, breaks = 30)

hist(nvda_model1$residuals, breaks = 30)
hist(nvda_model2$residuals, breaks = 30)
hist(nvda_model3$residuals, breaks = 30)

hist(msft_model1$residuals, breaks = 30)
hist(msft_model2$residuals, breaks = 30)
hist(msft_model3$residuals, breaks = 30)

hist(amzn_model1$residuals, breaks = 30)
hist(amzn_model2$residuals, breaks = 30)
hist(amzn_model3$residuals, breaks = 30)

```

Our histograms look normal and resembles a gaussian white noise, which is a clear indicator that our models performed well.

```{r, include = F}
g1 <- googl_model1$residuals
g2 <- googl_model2$residuals
g3 <- googl_model3$residuals

n1 <- nvda_model1$residuals
n2 <- nvda_model2$residuals
n3 <- nvda_model3$residuals

m1 <- msft_model1$residuals
m2 <- msft_model2$residuals
m3 <- msft_model3$residuals

a1 <- amzn_model1$residuals
a2 <- amzn_model2$residuals
a3 <- amzn_model3$residuals
```

<br>

### PACFS and ACF of Google model residuals

```{r, echo = F}
par(mfrow = c(2, 3))
acf(g1)
acf(g2)
acf(g3)
pacf(g1)
pacf(g2)
pacf(g3)
```

### PACFS and ACF of Nvidia model residuals

```{r, echo = F}
par(mfrow = c(2, 3))
acf(n1)
acf(n2)
acf(n3)
pacf(n1)
pacf(n2)
pacf(n3)
```

### PACFS and ACF of Microsoft model residuals

```{r, echo = F}
par(mfrow = c(2, 3))
acf(m1)
acf(m2)
acf(m3)
pacf(m1)
pacf(m2)
pacf(m3)
```

### PACFS and ACF of AMAZON model residuals

```{r, echo = F}
par(mfrow = c(2, 3))
acf(a1)
acf(a2)
acf(a3)
pacf(a1)
pacf(a2)
pacf(a3)
```

All in all, our pacf and acf charts for all of our stocks do resemble a white noise distribution, and we can continue forecasting our datasets.

<br>

# VII. Forecasting with Test Data

<br>

### Google Forecast

```{r echo=FALSE, message=FALSE, warning=FALSE, results = 'hide', fig.width=8, fig.height=6}
knitr::opts_chunk$set(cache = TRUE)

par(mfrow = c(3, 1))
sarima.for(gtr, n.ahead=52 ,p=1, d=0, q=2, P=1, D=1, Q=0, S=52, main="Forecasting Google Stock for 2019")
lines(gte, col = 'blue', type="b")

sarima.for(gtr, n.ahead=52 ,p=1, d=0, q=2, P=0, D=1, Q=1, S=52, main="Forecasting Google Stock for 2019")
lines(gte, col = 'blue', type="b")

sarima.for(gtr, n.ahead=52 ,p=1, d=0, q=2, P=1, D=1, Q=1, S=52, main="Forecasting Google Stock for 2019")
lines(gte, col = 'blue', type="b")
```

<br>

### Nvidia Forecast

```{r echo=FALSE, message=FALSE, warning=FALSE, results = 'hide', fig.width=8, fig.height=6}
knitr::opts_chunk$set(cache = TRUE)

par(mfrow = c(3, 1))
sarima.for(ntr, n.ahead=52, p=1, d=0, q=2, P=1, D=1, Q=0, S=52, main="Forecasting Nvidia Stock for 2019")
lines(nte, col = 'blue', type="b")

sarima.for(ntr, n.ahead=52, p=1, d=0, q=2, P=0, D=1, Q=1, S=52, main="Forecasting Nvidia Stock for 2019")
lines(nte, col = 'blue', type="b")

sarima.for(ntr, n.ahead=52, p=1, d=0, q=2, P=1, D=1, Q=1, S=52, main="Forecasting Nvidia Stock for 2019")
lines(nte, col = 'blue', type="b")
```

<br>

### Microsoft Forecast

```{r echo=FALSE, message=FALSE, warning=FALSE, results = 'hide', fig.width=8, fig.height=6}
knitr::opts_chunk$set(cache = TRUE)

par(mfrow = c(3, 1))
sarima.for(mtr, n.ahead=52, p=1, d=0, q=2, P=1, D=1, Q=0, S=52, main="Forecasting Microsoft Stock for 2019")
lines(mte, col = 'blue', type="b")

sarima.for(mtr, n.ahead=52, p=1, d=0, q=2, P=0, D=1, Q=1, S=52, main="Forecasting Microsoft Stock for 2019")
lines(mte, col = 'blue', type="b")

sarima.for(mtr, n.ahead=52, p=1, d=0, q=2, P=1, D=1, Q=1, S=52, main="Forecasting Microsoft Stock for 2019")
lines(mte, col = 'blue', type="b")
```

<br>

### Amazon Forecast

```{r echo=FALSE, message=FALSE, warning=FALSE, results = 'hide', fig.width=8, fig.height=6}
knitr::opts_chunk$set(cache = TRUE)

par(mfrow = c(3, 1))
sarima.for(atr, n.ahead=52, p=1, d=0, q=2, P=1, D=1, Q=0, S=52, main="Forecasting Amazon Stock for 2019")
lines(ate, col = 'blue', type="b")

sarima.for(atr, n.ahead=52, p=1, d=0, q=2, P=0, D=1, Q=1, S=52, main="Forecasting Amazon Stock for 2019")
lines(ate, col = 'blue', type="b")

sarima.for(atr, n.ahead=52, p=1, d=0, q=2, P=1, D=1, Q=1, S=52, main="Forecasting Amazon Stock for 2019")
lines(ate, col = 'blue', type="b")
```

<br>

# VIII. Results

<br>

**Google:** The SARIMA model applied to Google's stock data projected a consistent, gradual upward trend, without significant seasonal fluctuations. This model closely tracked the actual stock prices until the third quarter of 2019, at which point the actual stock prices experienced a sharper increase than anticipated by the model. This deviation suggests that while the model captured the general trend effectively, it was less responsive to more abrupt market shifts.

**Nvidia:** The forecast for Nvidia's stock was notably accurate in its early stages, closely mirroring actual stock values. However, a deviation from the predicted trajectory was observed in the third quarter of 2019. Despite this divergence, the model's performance was commendable, ranking as the second most accurate among the analyzed stocks.

**Microsoft:** The predictions for Microsoft's stock presented a significant deviation from actual stock performance. The model appeared to rely on the stock's previous steady growth trend, failing to anticipate the substantial price increase that commenced at the outset of 2019. This discrepancy highlights the model's limitations in adapting to sudden market changes.

**Amazon:** The forecast for Amazon's stock was the most accurate, with the model successfully capturing its stable growth trend throughout most of 2019. The true stock values largely fell within the predicted confidence intervals, underscoring the model's effectiveness. This accuracy may be partly attributed to the inherent stability of Amazon's stock compared to more volatile market entities.

It is important to note external factors that potentially influenced these forecasts. One significant event was the escalation of the US-China trade war in mid-July 2019, coinciding with the latter part of our training dataset. This geopolitical development had a pronounced impact on the technology sector, which is heavily reliant on manufacturing and labor resources from China. Such external factors, although not directly accounted for in our model, likely played a role in the deviations observed in our forecasts.

In summary, while our models demonstrated a capacity to track general stock trends effectively, their performance varied across different companies and they showed limitations in responding to rapid market changes and external geopolitical events.

<br>

# IX. Discussion

<br>

This study embarked on an in-depth examination of weekly stock prices from four prominent technology corporations over a decade (2010-2020). The primary aim was to evaluate predictive capabilities for the year 2019, utilizing ARIMA models as the central analytical tool.

Our comprehensive analysis model forecasts against actual stock performances, revealing varying degrees of prediction accuracy. Notably, Google's stock predictions aligned closely with real-world data until a significant rise in the third quarter of 2019, suggesting an underestimation of market dynamics. Nvidia's forecast mirrored actual trends up to a similar time frame, after which the prediction diverged. Microsoft's stock, however, presented the most substantial challenge, with the model failing to capture the increase observed in 2019. In contrast, Amazon's stock forecast emerged as the most accurate, with the predictive model effectively tracking its price trajectory throughout the year.

These outcomes suggest that while ARIMA models offer valuable insights, their precision varied considerably among the different stocks. Amazon's predictability positions it as a potentially reliable option for investors focusing on the technology sector. Nonetheless, the discrepancy observed, especially with Google and Microsoft, underscores the complexities inherent in stock market forecasting, especially in stocks that are much more volatile and susceptible to external factors. 

This analysis not only underscores the utility of statistical models in financial forecasting but also highlights their limitations within highly dynamic sectors like technology. The deviations observed reiterate the necessity of considering a broader range of factors, including economic, political, and global health events, in future forecasting endeavors.

Conclusively, while the precision of our ARIMA models did not entirely meet expectations, they provided significant insights into stock market behaviors, underscoring the importance of nuanced analysis in making informed investment decisions. More sophisticated research could expand the model's parameters to encompass a wider array of influencing factors, such as the external factors which were not included in this study, potentially enhancing the accuracy and reliability of stock market forecasts.

<br>

### References:

[1] Yahoo Finance. (2023). Amazon Stock History. Retrieved February 20, 2023, from https://finance.yahoo.com/quote/AMZN/history?p=AMZN

[2] Yahoo Finance. (2023). Microsoft Stock History. Retrieved February 20, 2023, from https://finance.yahoo.com/quote/MSFT/history?p=MSFT

[3] Yahoo Finance. (2023). Nvidia Stock History. Retrieved February 20, 2023, from https://finance.yahoo.com/quote/NVDA/history?p=NVDA

[4] Yahoo Finance. (2023). Google Stock History. Retrieved February 20, 2023, from https://finance.yahoo.com/quote/GOOGL/history?p=GOOGL

[5] Shumway, R. H., & Stoffer, D. S. (2017). *Time Series Analysis and Its Applications: With R Examples* (4th ed.). Springer.
